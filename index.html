<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Batch Normalization Blog</title>
</head>
<body>

<h2>Introduction</h2>
<p>Batchnorm has been empirically shown to allow deep neural nets to train faster and more stably (less sensitive to the choice of initialization). However, the exact theoretical benefit of the batch norm layer has always been a topic of debate. The main difficulty perhaps comes from the fact that NN has many moving parts so it is hard to put your finger down on the exact root problem the BN layer solves, and whether BN is the unique mechanism that solves it. The original paper from 2015 attributes the success to resolving the problem of internal covariate shift. In 2019, there is a new paper that argues, that instead of ICS, it is the fact that batch norm layer makes the optimization landscape smoother that justifies BN's success.</p>

<p>In this blog, we will go through the list of items:</p>
<ol>
  <li>What is batch norm and implement a simple neuron net with batch norm layer</li>
  <li>First benefit: preventing dead or saturated units</li>
  <li>Second benefit: Resolving the Internal Covariate Shift problem (and why it is not entirely true) (2015 paper)</li>
  <li>Third benefit: Smoothening the loss landscape (2019 paper)</li>
</ol>

<br/><br/>

<h2>Formal definition of batch normalization</h2>
<p>Batch norm is a mechanism that aims to stabilize the distribution of inputs to a network layer during the training phase. Specifically, the batch norm layer converts the first two moments of the neuron's input (denoted as y) to mean 0 and variance 1. The mean and standard deviation are calculated based on the current batch.</p>

<img width="729" alt="Screenshot 2024-02-25 at 11 00 54â€¯AM" src="https://github.com/Iancheung228/Iancheung228.github.io/assets/37007362/312f5c2e-0dad-49fd-8882-384737fdc998">

<p>In practice, a BN layer includes 2 learnable parameters (in green) for the output mean and variance for each column. This is done so that BN to maintain the expressive power of the original network, i.e. sometimes we do want the mean to not be zero, or the variance not be one.</p>

<br/><br/>

<h2>Example of PyTorch code for a 2-layered neuron net with batch norm</h2>
<pre>
<code>
# defining our layers
X_dim = 5                   # dimension of a training data point
n_hidden = 100              # the number of neurons in the hidden layer of this NN

g = torch.Generator().manual_seed(42) # for reproducibility
W1 = torch.randn((X_dim , n_hidden), generator=g)
W2 = torch.randn((n_hidden, X_dim),  generator=g)
b2 = torch.randn(X_dim,              generator=g)


# BatchNorm parameters
bngain = torch.ones((1, n_hidden))          # in green in the diagram of BN def 
bnbias = torch.zeros((1, n_hidden))         # in green in the diagram of BN def 
bnmean_running = torch.zeros((1, n_hidden)) # not trained using back prop (used at inference)
bnstd_running = torch.ones((1, n_hidden))   # not trained using back prop (used at inference)

parameters = [W1, W2, b2, bngain, bnbias]
for p in parameters:
  p.requires_grad = True

batch_size = 32
# minibatch construction
ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)
Xb, Yb = Xtr[ix], Ytr[ix]

# Linear layer
pre_act = embcat @ W1
## We don't need a bias term as the BN layer will get rid of the bias here
## pre_act has shape (batch_size, 100) where 100 is the number of neurons.
## For each neuron, we want to find the mean and std across all 32 training examples.

# BatchNorm layer-------------------------------------------------------------
bnmeani = pre_act.mean(0, keepdim=True)
bnstdi = pre_act.std(0, keepdim=True)
pre_act = bngain * (pre_act - bnmeani) / bnstdi + bnbias # each neuron will be unit gaussian for this batch of data
# running mean to use in validation, these running also get updated in the training phase, but these do not require any gradient
with torch.no_grad():
  bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani
  bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi
# -------------------------------------------------------------
# Non-linearity
h = torch.tanh(pre_act)            # non linear layer
logits = h @ W2 + b2               # output layer
loss = F.cross_entropy(logits, Yb) # loss function
</code>
</pre>

<h2>First benefit: preventing dead or saturated units</h2>
<p>Many activation functions, including Tanh are a squashing function, this means Tanh will remove information from the given input to the function. Specifically, if the input value (in absolute value) is too big, tanh will return 1/-1, which corresponds to the flat region in the tail end of the function. From a gradient pov, if we land on the flat region, the gradient would be 0, and virtually this will stop any gradient flowing through this neuron when we try to update the neuron's weight. In other words, if the neuron's output (in absolute value) is too big, no matter how you perturb the value of the neuron, it will not have any impact on the final loss, and hence the neuron will not get updated. We call this a dead neuron.</p>

<p>By adding a batch norm layer before the activation layer, we would force the input to take on a zero mean and unit variance distribution which prevents the landing on flat regions and subsequently dead neurons.</p>

<h2>Second benefit: Resolving the Internal Covariate Shift problem (and why it is not entirely true) (2015 paper)</h2>
<p>ICS is closely related to the concept of covariate shift, which is when the input-data distribution shifts over time. For example, we could use pre-covid's stock data to train a stock price prediction model, however, chances are the model will not be effective in predicting returns for post-COVID era, as the data distribution has changed substantially, for obvious reasons.</p>

<p>Now, adding the word "Internal" before "covariate shift", describes a closely related phenomenon that occurs in the training of a neural network, where the distribution of input for an individual layer, changes due to the update of the previous layers' weights.</p>

<p>Let me introduce a useful framework for thinking about neuron nets. We can think of a neural net as a function parameterized by weights. This function takes a given datapoint as input and outputs a prediction. The training of Neural Nets can be seen as solving an optimization problem, where we attempt to learn the optimal weight for the function in order to map our datapoint to the true label as closely as possible. In fact, we can break down the original optimization problem into solving a series of smaller, sequential optimization problems at a layer level.</p>

<p>That is, each layer is also a function that takes in an input (received from the previous layer) and produces an ouput (feeds to the next layer). The layer wise optimization problem has similar flavor, where we try to find good weights that map the input to the desired output. Precisly speaking the input here refers to the output from the previous layer, and the desired output is related to the accumulation of the gradient w.r.t the final loss from the later layers.</p>

<p>The ICS occurs when the input for the layer (output of the previous layer) changes drastically (due to weight update in the previous epoch) in every iteration of the training procedure.</p>

<p>Let's walk through an example for more clarity.</p>

<h2>Example</h2>
<p>Consider a neural network with 3 layers (each with 1 neuron) with no nonlinearity. Let's walk through how backpropagation will update the weights of the 3 layers, for 2 epochs <strong>a)</strong> \(i-1\) and <strong>b)</strong> \(i\).</p>

<p><strong>Notation</strong></p>
<ul>
<li> \(w_c\) denotes the weight of neuron c</li>
<li> \(z_c\) denotes the output of neuron c</li>
<li> L denotes the loss (common choice is \({\lvert\hat{y} - y \rvert}^2\)</li>
<li> \(\alpha \) is the learning rate</li>
</ul>

<p>... [more content]</p>

</body>
</html>
