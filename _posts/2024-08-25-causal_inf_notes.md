---
layout: post
title: Causal Inference Notes
date: 2024-08-25
---

In A/B testing, we compare two versions of a product: the control (the current version in production) and the treatment (the new version or variation being tested). The goal is to determine if there is a difference between the two versions with respect to a specific metric that matters to the business.

Ideally, we would test both versions on the entire population, and the results would be definitive. However, in practice, testing on the entire population is not feasible so we instead test on a sample of the population. This introduces variability, and to account for this, we rely on statistical methods to assess whether any observed differences are meaningful or simply due to random chance.



## High level steps of ab testing
1. Sample a subset of the population and randomly assign treatment and control to the randomization unit
2. Calculate the metric mean under treatment  $$\overline{Y^T}$$ and the metric mean under control $$ \overline{Y^C}$$.***Note, even if the metric Y does not follow a normal distribution, if the sample size is large enough, the mean of the metric will be normally distributed thanks to the central limit theorem.*** 
3. Calculate the difference in the sample mean $$ \Delta = \overline{Y^T} - \overline{Y^C} $$ where $$ \Delta $$ follows a normal distribution by property of adding 2 normally distributed random variable (r.v).
4. Calculate the observed z-score $$Z = \frac{\Delta}{\sqrt{Var}} $$ and eventually the p-value. The z-score is a standardized number that tells you how far away your observed data is from the mean.

We go in-depth on step 3 and 4
### 3) title
Again, since the sample mean has inherent randomness due to sampling the difference in means, denoted as $$\Delta$$ is also a random variable and follows a normal distribution. 
We have yet to determine the **mean** and **variance** of this normal distribution.It turns out that the mean of this distribution depends on our belief of the true difference. More specifically our belief of Null hypothesis and alternative hypothesis.

Under the **Null hypothesis**, we assert that the treatment mean equals the control mean and our observation of the r.v $$\Delta$$ (and after standardization) is generated from the null distribution following N(0,1).

Under the **Alternative hypothesis**, we assert that the difference in treatment mean and control mean is $$\delta$$, and our observation of the r.v $$\Delta$$ (and after standardization) is generated from the alternative distribution following N($$\delta$$,1)

***Note to standardize a normal distribution to have a variance of 1, we do $$ \frac{ \Delta - truth}{appropriate s.d.} $$.***

### 4) title
We then calculate the observed z-score: Z = $$ \frac{\Delta}{\sqrt{var(\Delta)}}$$ (link to step 4).


Under the further assumption that the Null is true (i.e. mean = 0), the z-score now follows a standard normal distribution since we can rewrite it as $$ \frac{ \Delta - {\color{red}0}}{appropriate s.d.} $$ 

We now say that the Z score is a realization of the standard normal r.b. $$\zeta$$.

$$ \text{p-value} = P( |Z| \geq z_{\alpha/2} |H_0) = P(| \zeta | \geq z_{\alpha/2}) = \alpha = 5\% $$

**P-value** is defined as the probability of observing a more extreme test statistic, under the assumption that the **null hypothesis** is true. 

***Type 1 error ($$alpha$$):*** concluding there is a significant difference when there is no real difference. A common choice of $$\alpha$$ is 5%.

***Power (1-beta):*** is defined as the probability of rejecting the null when the null is in fact false (WHYWHYWHYWHYWHWYHWYHWYH why is this binary? and the alternative is true). Equivalently, it is the prob of not making a type II error.

$$ \text{power} =  P(\text{p-value} < \alpha | H_A) $$

**Type 2 error:** concluding there is no significant difference when there is one.

## P-value/type 1 error vs Power/type 2 error

<div style="display: flex; justify-content: space-between; align-items: flex-start;">

  <!-- First Plot (Null hypothesis) -->
  <figure id="null-graph" style="text-align: center; width: 45%; margin: 0;">
    <h3 style="text-align: center;">Type I Error</h3>
    <img src="https://github.com/user-attachments/assets/e292e422-1b9c-4af0-8786-fd9137c174f1" alt="IMG_E39688680BFC-1" width="100%" />
    <figcaption> Under the assumption that null is true (i.e our observation is generated by the null distribution). </figcaption>
  </figure>
  
  <!-- Second Plot (Alternative hypothesis) -->
  <figure id="alternative-graph" style="text-align: center; width: 45%; margin: 0;">
    <h3 style="text-align: center;">Type II Error</h3>
    <img src="https://github.com/user-attachments/assets/3f8fbd92-a02f-4395-b395-595d60499c36" alt="IMG_5665639446F5-1" width="100%" />
    <figcaption> Under the assumption that the alternative is true (i.e our observation is generated by the alternative distribution). </figcaption>
  </figure>

</div>




In the figure on the left, an error is made if we decide to reject the Null, which occurs when our observation lies to the right of the critical value *. While in the right figure, an error is made if we decide to not reject the Null, which occurs when our observation lies to the left of *. 

It's important to note that we can reduce the probability of a Type II error by shifting the decision boundary (i.e., the value of $$\alpha$$ to the right. However, doing so would increase the probability of a Type I error, since a larger critical region would make it easier to reject the null hypothesis.




[Go to Power Graph](#null-graph)


## What levers affect power

From the above diagram we see that there are 3 levers that affect power:

1) $$ \alpha $$: To increase power, we can increase our $$ \alpha $$, but again there is a trade-off between making a Type I error (false positive) and a Type II error (false negative).

2) $$ n $$: You can increase the sample size. Both the Null and Alternative distributions will become narrower in shape, leading to less overlap between the 2 distributions. 

3) $$ \delta $$: If the true parameter is further away from the null hypothesis value, there will again be less overlap between the 2 distributions and power increases.

### Practically how do we pick what value of $$\delta$$ to use for the alternative hypothesis?
Given a significance level α, we are typically interested in determining the sample size required to achieve a desired statistical power. However, the choice of the alternative hypothesis still presents a challenge, as we don't know the true effect size δ under the alternative hypothesis.

What we do know, however, is the minimum effect size 𝛿 that would be considered meaningful in the context of the business or the specific problem at hand—often referred to as the **minimum detectable effect (MDE)**. This is the smallest effect size that is practically significant, and it is commonly used when calculating the required sample size.



### Hypothesis Testing Error Matrix

|                       | $$H_0$$ is true                   | $$H_A$$ is true                   |
|:-----------           |:------------:                     |:------------:                  |
| Don't reject Null     | ✅  (1-$$\alpha$$)                | Type II error $$\beta$$    |
| Reject Null           | Type I error $$\alpha$$           | ✅  Power (1-$$\beta$$)         |






## Examplep of how to calculate the sample size needed for a desired power, given an alpha level and true alternative hypothesis of $$ \alpha = \delta_A >0 $$

Consider the following setup:
- The metric of interest is conversion, where the object either successfully converted or failed to convert
- The sample variance for a Bernoulli variable would be $$ \sigma^2 = \frac{ \mu (1-\mu)} {N} $$
- The true $$\Delta$$ under the alternative hypothesis is $$\delta_A$$ and is greater than 0
- The Null and alternative distributions are parameterized by $$N(\mu_1, \sigma_1^2)$$ and $$N(\mu_2, \sigma_2^2)$$ with sample sizes $$N_1$$ and $$N_2$$. 
- $$\Delta$$ = $$N(\mu_1, \sigma_1^2) - N(\mu_2, \sigma_2^2) = N\left(\mu_2 - \mu_1, \sigma_1^2 + \sigma_2^2 \right)$$


**Goal** We wish to obtain a power of 1- $$\beta$$, we want to reject the null at least 100(1- $$\beta$$)% of the time assuming the alternative hypothesis is true.

$$
1 - \beta \leq P\left( Z > z_{\alpha} \, \Big| \, \text{alternative} \right) 
= P\left( \frac{ \widehat{\mu_1} - \widehat{\mu_0} }{ \sqrt{\mu(1 - \mu) \left( \frac{1}{n} + \frac{1}{n} \right) } } > z_{\alpha} \, \Big| \, \text{alternative} \right)
$$


### Note:
1) The z score follows a standard normal only under the Null hypothesis, and not under the alternative. We have to rearrange the above expression to obtain something normally distributed.
2) Under alternative $$ Var(\widehat{\mu_1} -  \widehat{\mu_0}) = \frac{\mu_1(1- \mu_1)}{n} +  \frac{\mu_0(1-\mu_0)}{n} $$


$$
\begin{aligned}
1 - \beta &\leq P\left( \widehat{\mu}_1 - \widehat{\mu}_0 > z_{\alpha} \sqrt{\mu(1 - \mu) \left( \frac{1}{n} + \frac{1}{n} \right)} \Big| \, \text{alternative} \right) \\
          &= P\left( \frac{(\widehat{\mu}_1 - \widehat{\mu}_0) - \delta_A}{ \frac{\mu_1(1 - \mu_1)}{n} + \frac{\mu_0(1 - \mu_0)}{n} } > \frac{z_{\alpha} \sqrt{\mu(1 - \mu) \left( \frac{1}{n} + \frac{1}{n} \right)} - \delta_A}{ \frac{\mu_1(1 - \mu_1)}{n} + \frac{\mu_0(1 - \mu_0)}{n} } \Big| \, \text{alternative} \right) \\
          &= P\left( Z^* > \frac{z_{\alpha} \sqrt{\mu(1 - \mu) \left( \frac{1}{n} + \frac{1}{n} \right)} - \delta_A}{ \frac{\mu_1(1 - \mu_1)}{n} + \frac{\mu_0(1 - \mu_0)}{n} } \Big| \, \text{alternative} \right) \\
\end{aligned}
$$


Now the quantity on the left of the inequality, $$Z^* \sim N(0, 1)$$ under the alternative hypothesis



$$
z_{1 - \beta} \leq \frac{
    z_{\alpha} \sqrt{\mu(1 - \mu) \left( \frac{1}{n} + \frac{1}{n} \right)} - \delta_A
}{ 
    \frac{\mu_1(1 - \mu_1)}{n} + \frac{\mu_0(1 - \mu_0)}{n} 
} \quad \text{since} \quad Z^* \sim N(0, 1)
$$

Then, you can simply continue the algebraic manipulation and isolate for n.




### Aside $$ \text{P-value} \neq P(H_0 | data) $$
A common misconception is that the p-value represents the probability that the null hypothesis ($$H_0$$) is true, given the observed data. I.e. a p-value of $$5\%$$ means the null hypothesis has $$5\%$$ chance of being true $$ P(H_0 \mid \text{p-value} = 0.05) = 0.05 $$. 

Or something like: a p-value of $$5\%$$ means there is a $$5\%$$ chance of us making a false positive. 
$$ P(H_0 \mid \text{p-value} = 0.05) = 0.05 $$

The above interpretations are wrong because the p-value doesn't tell us the probability of the Null being true.

While this is an expression of great interest, it is important to note that we cannot directly calculate this probability without knowing the prior probability of $$H_0$$.

$$P(H_0 \mid \text{data})$$ and the p-value are related by the Bayes' theorem:

$$
\begin{aligned}
P(H_0 \mid \text{data}) &= \frac{ P(\text{data} \mid H_0) \times P(H_0)}{P(\text{data})} \\
&= \text{p-value} \times \frac{P(H_0)}{P(\text{data})}
\end{aligned}
$$

$$ P(H_0 |Data) = \frac{ P(Data|H_0)P(H_0)}{P(Data|H_0)P(H_0) + P(Data|H_1)P(H_1)} $$

It does align with our intuition: a lower p-value suggests that the null hypothesis is less likely to be true.




In reality, the p-value is the probability of observing a result as or more extreme than what was observed, under the assumption that the null hypothesis is true.
$$ \text{p-value} = P(\delta \text{ observed or more extreme} \mid H_0 \text{ is true}) $$


### What is so bad about low power, does it matter as long as the p-value is low? ###

Even if the p-value is statistically significant, if the experiment is low-powered to begin with there is still a high probability of the Null hypothesis to be true.

**False positive risk (FPR)** is the probability that the Null hypothesis is true while we decide to reject the null hypothesis (p-value < 5%). Mathematically it is represented by $$ P(\text{Null is True} \mid \text{p-value} < 0.05) $$


Let
- SS be a statistically significant result i.e. p-value < 0.05
- $$\alpha$$ be the threshold to declare statistical significance.
- $$\beta$$ be the Type II error rate.
- $$\pi$$ be the prior probability of the null hypothesis.


$$
\begin{aligned}
P(H_0 \mid SS) &= P(SS \mid H_0) \cdot \frac{P(H_0)}{P(SS)} \\
&= \frac{ P(SS \mid H_0) P(H_0)}{P(SS \mid H_0) P(H_0) + P(SS \mid H_1) P(H_1)} \\
&= \frac{\alpha \cdot \pi}{\alpha \cdot \pi + (1-\beta)(1-\pi)}
\end{aligned}
$$


If the power $$1- \beta$$ is low the denominator will be small and the entire fraction (FPR) will be high. This means when power is low the probability that the Null hypothesis is true is small (even when p-value is statistically significant)

punch line: with low power even if you have very small p value the FPR is very high.


















<!-- MathJax -->

<script type="text/javascript"

  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

</script>




