---
layout: post
title: Causal Inference Notes
date: 2024-08-25
---

Intuition of ab testing

In ab tests, we have 2 versions of a product. One is currently in production (control) and the other is the contender (treatment). We wish to determine whether there is a difference between the control vs treatment for a metric that we care about. In an ideal world, we would perform the experiment on the entire population and the result would be deterministic, however in reality we could only perform the experiment over a sample of the population. We need statistical testing techniques to quantify this variability.

*** High level steps of ab testing***
1. Sample a subset of the population and randomly assign treatment and control to the randomization unit
2. Calculate the mean under treatment $$Y^T$$ and the mean under control $$Y^C$$. They will be the unbiased estimate of the population treatment mean and population control mean. Note, even if the metric Y does not follow a normal distribution, if the sample size is large enough, the mean of the metric will be a normal distribution thanks to the central limit theorem. 
3. Calculate the difference in sample mean $$ \Delta = Y^T - Y^C $$ note $$ \Delta $$ follows a normal distribution as adding 2 normally distributed r.v will result in another normally distributed r.v.
4. Use that to calculate our test statistics and eventually the p-value.

We will expand on step 4. 

P-value is the probability of observing a more extreme test statistic, under the assumption that the null hypothesis is true. 
$$ P(data | H_0) $$

The Null hypothesis: the null distribution is normally distributed with mean around 0. This distribution is for the r.v $$ \Delta$$, it is a random variable because we run the ab test on a sample of the population. If we perform ab test many times, the random variable would take on a different value each time, and it follows this null distribution. each observation is one realization.

what is the null distribution? It is a hypothetical distribution assuming the treatment mean is no different from the control mean. If we have access to the entire population, this belief won't be represented by a distribution but instead by the number 0. But since we can only sample a subset of the population, if we repeat the same experiment many times, we would expect some deviation in the observed outcome. We say our observed statistic is just one realization of the Null distribution. 


A common misconception is the p-value captures the probability the Null hypothesis is true given the data observed. 
$$ P(H_0 | data) $$



we see below how the 2 are related:

$$
\begin{aligned}
P(H_0 \mid \text{data}) &= \frac{P(\text{data} \mid H_0) P(H_0)}{P(\text{data})} \\
&= \text{p-value} \times \frac{P(H_0)}{P(\text{data})}
\end{aligned}
$$

We do see that however, generally speaking the lower the p-value the smaller the chance H_0 is true.


We do $$ \frac{MLE - truth}{appropriate s.d.} $$ to transform into a standard normal distribution. 
It is also called the t-statistics  $$ \frac{\Delta}{\sqrt{var(\Delta)}}$$ where $$\Delta = \overline{Y^t} - \overline{Y^c} $$ The T statistic is just the normalized version of $$\delta$$ We compute p-values with the assumption that the t stat follows a normal distribution, further under the null hypothesis it has a mean of 0 and variance 1. 



***P-value and type 1 error***


<div style="display: flex; justify-content: space-between;">
  <figure id="null-graph" style="text-align: center; width: 45%;">
    <img src="https://github.com/user-attachments/assets/e5d0ca28-7e1a-42f4-b1e0-4294d8f3b367" alt="IMG_E39688680BFC-1" width="100%" />
    <figcaption> Under the assumption that the null hypothesis is true and that our observation is generated by the null distribution. We made an error if we decide to reject the Null, which occurs when our observation lies to the right of $$\alpha$$ </figcaption>
  </figure>
  <figure id="alternative-graph" style="text-align: center; width: 45%;">
    <img src="https://github.com/user-attachments/assets/cbf6e3fc-a4c1-4e5c-acd1-a39ae1cfc9ee" alt="IMG_5665639446F5-1" width="100%" />
    <figcaption> Under the assumption that the alternative hypothesis is true and that our observation is generated by the alternative distribution. We made an error if we decide to not reject the Null, which occurs when our observation lies to the left of $$\alpha$$ </figcaption>
  </figure>
</div>

[Go to Power Graph](#null-graph)

***Type 1 error:*** concluding there is a significant difference when there is no real difference.A common choice is 5%.

Under null Z, the observed test statistic comes from a standard normal distribution and is simply a realization of the squiggle Z which is the standard normal random variable

$$ P( |Z| \geq z_{\alpha/2} |H_0) = P(|Z| \geq z_{\alpha/2}) = \alpha = 5\% $$

***Power (1-beta):*** The probability of rejecting the null when the null is indeed false. Equivalently, it is the prob of not making a type II error.

$$ power =  P(p-value <0.05 | H_A) $$

Type 2 error: concluding there is no significant difference when there is one.


|                       | Center Align                      | Right Align                   |
|:-----------           |:------------:                     |------------:                  |
| Don't reject Null     | ✅  (1-$$\alpha$$)            | Type II error 100$$\beta$$    |
| Reject Null           | Type I error $$\alpha$$           | ✅  (1-$$\beta$$)         |






***How to calculate the sample size needed for a desired power, given an alpha level and true alternative hypothesis***

To obtain a power of 1- $$\beta$$, we want to reject the null at least 100(1- $$\beta$$)% of the time assuming the alternative hypothesis is true.


$$
1 - \beta \leq P\left( Z < z_{\alpha} \, \Big| \, \text{alternative} \right) 
= P\left( \frac{ \widehat{\pi_1} - \widehat{\pi_0} }{ \sqrt{\pi(1 - \pi) \left( \frac{1}{n} + \frac{1}{n} \right) } } < z_{\alpha} \, \Big| \, \text{alternative} \right)
$$


But $$ Z \sim N(0, 1) $$ is only true when we condition on Null, but not under the alternative. We have to rearrange to obtain something normally distributed.
Also note that under alternative $$ Var(\widehat{\pi_1} -  \widehat{\pi_0}) = \frac{\pi_1(1- \pi_1)}{n} +  \frac{\pi_0(1-\pi_0)}{n} $$





$$
\begin{aligned}
1 - \beta &\leq P\left( \widehat{\pi}_1 - \widehat{\pi}_0 \leq z_{\alpha} \sqrt{\pi(1 - \pi) \left( \frac{1}{n} + \frac{1}{n} \right)} \Big| \, \text{alternative} \right) \\
          &= P\left( \frac{(\widehat{\pi}_1 - \widehat{\pi}_0) - \delta_A}{ \frac{\pi_1(1 - \pi_1)}{n} + \frac{\pi_0(1 - \pi_0)}{n} } \leq \frac{z_{\alpha} \sqrt{\pi(1 - \pi) \left( \frac{1}{n} + \frac{1}{n} \right)} - \delta_A}{ \frac{\pi_1(1 - \pi_1)}{n} + \frac{\pi_0(1 - \pi_0)}{n} } \Big| \, \text{alternative} \right) \\
          &= P\left( Z^* \leq \frac{z_{\alpha} \sqrt{\pi(1 - \pi) \left( \frac{1}{n} + \frac{1}{n} \right)} - \delta_A}{ \frac{\pi_1(1 - \pi_1)}{n} + \frac{\pi_0(1 - \pi_0)}{n} } \Big| \, \text{alternative} \right) \\
\end{aligned}
$$


now the quantity on the left of the inequality is now $$ \sim N(0, 1)$$ under a true effect of $$ \delta_{\alpha}$$



$$
z_{1 - \beta} \leq \frac{
    z_{\alpha} \sqrt{\pi(1 - \pi) \left( \frac{1}{n} + \frac{1}{n} \right)} - \delta_A
}{ 
    \frac{\pi_1(1 - \pi_1)}{n} + \frac{\pi_0(1 - \pi_0)}{n} 
} \quad \text{since} \quad Z \sim N(0, 1)
$$

Then, you can simply continue the algebraic manipulation and isolate for n.



***What levers affect power***
From the above calculation of power, we see there are 3 levers that affect power:

1) $$ \alpha $$: To increase power, we can increase our alpha, but this is a trade-off between making a Type I error (false positive) and a Type II error (false negative).

2) $$ n $$: You can increase the sample size. Both distributions will become narrower, leading to less overlap and more power.

3) $$ \delta $$: If the true parameter is further away from the null hypothesis value, the power increases.


Given a alpha level, we are usually interested in finding the sample size needed to reach a certain power. However, a seemingly arbitrary choice of alternative hypothesis remains? Indeed we don't know the true effect $$\delta$$ under the alternative hypothesis.


However, we do know the size of $$\delta$$ that would matter under business context or that is of practical significance. The common practice is to use the smallest $$\delta$$ that is practically significant (also called the minimum detectable effect).


### What is so bad about low power, does it matter as long as p-value is low? ###

Common misunderstandings of p-value:

a) A statistically significant result with a p-value of 0.05 means that the null hypothesis has a 5% chance of being true. 
$$ P(H_0 \mid \text{p-value} = 0.05) = 0.05 $$

b) Equivalently, it's the same as saying: a p-value of 0.05 has a 5% chance of being a false positive. 
$$ P(H_0 \mid \text{p-value} = 0.05) = 0.05 $$


**Note**: The p-value is not the probability that the null hypothesis is true, nor is it the probability of a false positive. It is the probability of observing the data (or something more extreme) under the assumption that the null hypothesis is true.

This is wrong because the p-value doesn't tell us the probability of the Null being true.


In reality, the p-value is the probability of observing a result as or more extreme than what was observed, under the assumption that the null hypothesis is true.

$$ P(H_0 |Data) = \frac{ P(Data|H_0)P(H_0)}{P(Data|H_0)P(H_0) + P(Data|H_1)P(H_1)} $$

$$ \text{p-value} = P(\delta \text{ observed or more extreme} \mid H_0 \text{ is true}) $$


**False positive risk (FPR)** is the probability that the Null hypothesis is true while we make the decision to reject the null hypothesis (p-value < 5%). Mathematically it is represented by $$ P(\text{Null is True} \mid \text{p-value} < 0.05) $$

Let:

<div style="text-align: left;">
$$SS \quad \text{is a statistically significant result.}$$
$$\alpha \quad \text{is the threshold to declare statistical significance.}$$
$$ \beta \quad \text{is the Type II error rate.} $$
$$ \pi \quad \text{is the prior probability of the null hypothesis.} $$
</div>

$$
\begin{aligned}
P(H_0 \mid SS) &= P(SS \mid H_0) \cdot \frac{P(H_0)}{P(SS)} \\
&= \frac{ P(SS \mid H_0) P(H_0)}{P(SS \mid H_0) P(H_0) + P(SS \mid H_1) P(H_1)} \\
&= \frac{\alpha \cdot \pi}{\alpha \cdot \pi + (1-\beta)(1-\pi)}
\end{aligned}
$$










However, the significance calculation makes a critical assumption that you have probably violated without even realizing it: that the sample size was fixed in advance. If instead of deciding ahead of time, “this experiment will collect exactly 1,000 observations,” you say, “we’ll run it until we see a significant difference,” all the reported significance levels become meaningless. 



punch line: with low power even if you have very small p value the FPR is very high.











Consistency:
For each individual, one of their counterfactual outcomes is actually factual (one that corresponds to the treatment value that the individual received).
The counterfactual outcome equals the observed outcome and can be expressed as $$Y^A=Y$$ where $$Y^A$$ denotes the counterfactual $$Y^a$$ evaluated at the observed treatment value a.

Problem with I.C.E
ICE is defined as a contrast of the values of counterfactual outcomes, but only one of those outcomes is ever observed for each individual. All other counterfactual outcomes apart from the one 
corresponding to the actual treatment value remain unobserved. Hence individual effects cant be identified

We go to 1.2 average causal effects
Average Causal Effect in the population: An ACE of treatment A on outcome Y is present if 
$$ Pr[Y^{a=1} =1] \neq Pr[Y^{a=0} = 1] $$ in the population of interest (this is only for binary outcomes)

the more general one is $$ E[y^{a=1}] \neq E[y^{a=0}] $$

Aside: The absence of ACE does not imply the absence of ICE

## 1.4 Random variability 
There are 2 main sources of imperfect irl
1) Sampling variability: In practice, we collect data on a sample of the population of interest. Even if we know the counterfactual outcomes in this sample population, we still do not have the exact risk proportion for the superpopulation. That is: we use (consistent estimator) sample proportion $$\hat{Pr}[Y^{a=0} = 1]$$ to estimate the super-population probability $$ Pr[Y^a =1] $$under treatment value a. We call it a consistent estimator because as number of indiv in sample grows, we expect estimate and true value to grow smaller as the error due to sampling variability obeys the law of large number. Since the super population prob cant be computed but only consistently estimated, we need to have a statistical procedure to evaluate the empirical evidence of the causal null hypothesis.

2) The value of an individual's counterfactual outcomes is not fixed either. In previous examples, Zeus would have a 100% chance of dying if treated and would have 0% chance dying if untreated. The value of the counterfactual outcomes is deterministic for each individual. In real life, counterfactual outcomes are often stochastic.

   

## 1.5 Association vs Causation

Association: 2 variables are associated if information about one tells you something about the value of the other
Causation: 2 variables are causally related if manipulating the level of one has the potential to change the level of others (holding all else fixed)

causal effect for an individual: The treatment A has a causal effect on an individual's outcome Y if $$Y^{a=1} \neq Y^{a=0}$$ for the individual

$$ E[Y^{1}_{i}]$$ is generally distinct from $$ E[Y_i|A_i =1] $$
former is defined as a prior and averaged across the entire population (only god knows)
Latter is defined after treatment assignment,  and the average among those who receive treatment $$A_i$$ = 1

The conditional probability 
$$ Pr[Y=1 | A=1] $$
is defined as the proportion of individuals that developed the outcome Y among those individuals in the population of interest that happened to receive treatment value a. 

We say treatment A and outcome Y are independent if 
$$ Pr[Y=1 | A=1] = Pr[Y=1 | A=0] $$,
that A is not associated with Y, or that A does not predict Y. Conversely we say that treatment A and outcome Y are dependent or associated

![Screenshot 2024-08-27 at 9 39 23 PM](https://github.com/user-attachments/assets/8700f062-3563-48e1-96c2-81e6c50fc510)

The definition of causation implies a contrast between all individuals treated vs all individuals untreated. Association implies a contrast between the treated and the untreated of the original diamond.

Put another way, inference about causation is concerned with the counterfactual worlds, like "What would be the risk if everybody been treated/untreated?" Inference for association is concerned with questions in the actual world, like "What is the risk in the treated/untreated?"

Association uses a conditional probability, namely being conditioned on having actually received treatment value a (A=a)
causation is unconditional, or marginal probability, the risk of $$Y^a$$ in the entire population.

Therefore, association is defined by a different risk in two disjoint subsets of the population determined
by the individuals’ actual treatment value (A = 1 or A = 0), whereas causation is defined by a different risk in the same population under two different treatment values (a = 1 or a = 0). 



The bottom line is, that causal inference requires unattainable counterfactual data, but all we can ever expect is real world data. The question then becomes under what conditions can real-world data be used for causal inference?


# Section 2: Randomization
Again irl, we don't know both potential outcomes, we only know the observed outcome Y under the treatment value A that the individual happened to receive. Only one of the 2 counterfactual outcomes is known for each individual: the one corresponding to the treatment level that he actually received. The data are missing for the other counterfactual outcomes.

Randomized experiments, still generate data with missing values of the counterfactual outcomes, however, randomization ensures that those missing values occur only by chance and hence causal questions could be consistently estimated.

Suppose we assign an individual to the white group if it is a tail, and to the grey group if it's a head. Then we ask our research assistant to administer the treatment, to give the white group the treatment and the grey group a placebo; we then proceed to collect the data. 


It wasn't until the next day that you found out they had misinterpreted your instructions and gave treatment to the grey group instead. This reversal of treatment DOES NOT affect our conclusions! 


When group membership is randomized, which particular group received the treatment is irrelevant for the value of 
$$ Pr[Y=1 |A=1]$$ or $$Pr[Y=1 |A=0]$$ 
Formally we say the groups are exchangeable.


Exchangeability: 
$$ Pr[Y^a = 1  | A=1] = Pr[Y^a = 1  | A=0] = Pr[Y^a =1]$$ 
We say  the counterfactual risk under treatment value a (both when a=1 and a =0) is the same in both groups A = 1 and A = 0. Phrasing differently, the actual treatment A does not predict the counterfactual outcome, and the actual treatment are independent for all values a. $$ Y^a \perp A$$

In the presence of exchangeability, the observed risk in treated within the white group would equal the counterfactual risk under treatment in the entire population!

Another attempt to explain exchangeability $$ Y^a \perp A$$. The counterfactual outcome $$Y^a$$ is like your genetics, it encodes deterministically how you will react if you took the treatment, before treatment A was randomly assigned. Another point is that we only learn about the value of the counterfactual/genetic makeup $$Y^a$$ only after the treatment A is given and only if A=a.

Difference between $$ Y^a \perp A $$ and $$ Y \perp A$$
In a randomized experiment in which exchangeability holds and we find the treatment has a causal effect on the outcome, then 
$$ Y \perp A$$ does not hold.

## Section 2.2 Conditional randomization 
In the example in Hernan's book, if you are in critical condition, you will be more likely to be assigned treatment than not. That is, we no longer have marginal exchangeability. The only thing conclusive is the observed risks equal the counterfactual risks WITHIN the group of all critical conditions.

1a. $$ Pr[Y^{a=1} = 1 \mid L =1] = Pr[Y=1 \mid L=1, A=1] $$

1b. $$ Pr[Y^{a=0} = 1 \mid L =1] = Pr[Y=1 \mid L=1, A=0] $$

2a. $$ Pr[Y^{a=1} = 1 \mid L =0] = Pr[Y=1 \mid L=0, A=1] $$

2b. $$ Pr[Y^{a=0} = 1 \mid L =0] = Pr[Y=1 \mid L=0, A=0] $$


We see that a conditionally randomized experiment is simply a combination of 2 separate marginally randomized experiments: one conducted in the subset of individuals in critical condition, the other in the subset of individuals in critical condition. Within each subset, the treated and the untreated are exchangeable.

Definition of Conditional exchangeability: $$Y^a \perp A|L$$ for all a.

Or Equivalently: 
$$ Pr[Y^a = 1 |A=1,L=1] = Pr[Y^a = 1 |A=0,L=1]$$

conditional randomization does not guarantee unconditional (marginal) exchangeability, it does guarantee conditional exchangeability within levels of the variable L.

## 2.3 Identification under conditional randomization: Standardization
The question is under conditional randomization, can we write the counterfactual in terms of observed data? (whether it's identifiable)

Suppose we want to calculate the causal risk ratio
$$ \frac{Pr[Y^{a=1} = 1]} {Pr[Y^{a=0} = 1]} $$

The numerator is the risk if all individuals in the population had been treated, we can find this with the weighted average of risk of each group, where weight is proportional to its size L.

Recall in conditional exchangeability
$$
\begin{aligned}
E[Y^1] &= \mathbb{E}_L(\mathbb{E}_{Y|L} [Y^1|L]) \quad (\text{by total expectation}) \\
       &= \sum_{\ell} E[Y^1 \mid L=\ell] \cdot P(L=\ell) \quad  \\
       &= \sum_{\ell} E[Y^1 \mid A=1, L=\ell] \cdot P(L=\ell) \quad (\text{by conditional exchangeability}) \\
       &= \sum_{\ell} E[Y \mid A=1, L=\ell] \cdot P(L=\ell) \quad (\text{by consistency}) \\
\end{aligned}
$$

## 2.4 Identification under conditional randomization: Inverse Probability Weighting

$$ E[Y^a] = E[\frac{I(A=a)}{f(A|L)} Y] $$

We create 2 simulations (2 trees) of what would have happened had all individuals in pop been treated, and untreated respectively. These simulations are correct under cond exchange. Then we POOL the 2 trees, to create a hypothetical population of size 2n in which every individual appears as treated AND untreated. This 2n population is called the pseudo population.

Given C.E in original population, the treated and untreated are MARGINALLY exchangeable in the pseudo pop because L is independent of A. That is, the associational risk ratio in the pseudo population equals the causal risk ratio in BOTH the pseudo pop and org pop.


Both standardization and IPW can be viewed as procedures to build a new tree in which all individuals receive treatment a.
They differ by using different set of probabilities to build the counterfactual tree; IPW uses the conditional prob of treatment A given covariate L, while standarization uses prob of covariate L and the conditional probability of outcome Y given A and L

Both simulate what would have been observed if L had not been used to decide the probability of treatment

## Power Analysis:
definition: Power (1- $$\beta$$) = The probability of rejecting the Null hypothesis when $$H_A$$ is in fact true.
The higher the power, the less likely you will make a false negative error. (type ii error, incorrectly failing to reject null)

definition: Alpha = The probability of rejecting the Null hypothesis when &&H_o$$ is in fact true. (type 1 error, incorrectly rejecting null)

Relationship between $$ /alpha$$, $$/beta$$ and sample size.
Sample size calculation:
For a fixed level of alpha, what sample size would be needed to guarantee a power of P?

Power calculation:
For a given sample size and clinically important treatment effect, what is the power of the study?

Example
Suppose we want to test $$H_0 : \pi_1 = \pi_0 $$ against $$H_A: \pi_1 \neq \pi_0$$ using risk difference. The score test for risk difference would be 

$$ Z = \frac{\hat{\pi_1} - \hat{\pi_0} }{\sqrt{\hat{\pi}(1-\hat{\pi})( \frac{1}{n} + \frac{1}{n})}}  ~ N(0,1)$$

Under a one-sided test, we reject the Null hypothesis when $$ Z < z^* $$ and we want an alpha of $$\alpha$$ means

$$ P_0(Z-z^*) = \alpha $$


To obtain a desired power of 1- $$\beta$$ we want to reject Null 1- $$\beta$$% of the time given Non null is indeed True. That is we want

1- $$\beta$$ $$ \leq P_A (Z<z)$$


## Chapter 4:
**def** effect modifier: We say V is a modifier of the effect of A on Y when the average causal effect of A on Y varies across levels of V.

Additive effect modification:
$$ E[Y^{a=1} -Y^{a=0} | V= 1] \neq E[Y^{a=1} -Y^{a=0} | V= 0] $$
Note the presence of effect modification depends on the effect measure being used.
We also only consider variables V that are NOT affected by treatment A as effect modifiers.

**def** qualitative effect modification: when the average causal effect in the subsets are in the opposite direction.

properties: with QEM, additive effect modification IFF multiplicative effect modification.
in the absence of QEM, you can find effect modification on one scale but not on the other.

how EM could exist in one scale but not other

We do not need conditional exchangeability to identify effect modification.


one thing to note is even if we found V (nationality) modifies effect of heart transplant A on risk of death Y, we DO NOT know the causal mechanism involved in the effect modification. Thus the term effect modification by V doesnt necessarily imply V plays a causal role in the modification of the effect. In our example, it is possible that nationality is simply a marker for the causal factor that is truly responsible for the modification of effect.

This contrasts with interaction, which DOES attribute a causal role to the variables involved.

## 4.3 Why should we care about effect modification:
1) If we found V is indeed an effect modifier from our study, we need to know the make up of V in the population for the findings of the study to be useful. For example, the average causal effect in the population was harmful in women but beneficial in men (and the proportion of women and men were the same such that they cancel out). Our conclusion would be different had we conducted our study where theres more women. That is the average causal effect in the population depends on the distribution of other effect modifiers in the population. In reality, our discussion thus far is really: the average causal effect of treatment A on outcome Y in a population **with a particular mix** of causal effect modifiers.
2) The extrapolation of causal effects computed in one population to a second population is referred to as transportability of causal inferences across population. Again the finding from one population might not translate to another population with a different distribution of effect modifiers.

In fact we are making a huge assumption that there all effect modifiers (including those unknown or unmeasured) are the same between the 2 populations. these unmeasured effect modifiers are not variables needed to achieve exchangeability, but just risk factors for the outcome. in general the transportabiltiy of effects across population is a more difficult problem than the identification of causal effects in a single population. It is an unverifiable assumption.

fine points:
- additive, but not multiplicative, effect modification is the appropriate scale to identify the groups that will benefit most from intervention. in the absence of additive effect modification, learning that there is multiplicative effect modification may not be very helpful for decision making.


## 4.4 Stratification as a form of adjustment

Stratified analysis is the natural way to identify effect modification. To see if V is a modifier, we compute the average causal effect of A on Y, for each stratum of V. Again, we can do this in both marginally exchangible studies or conditionally exchangible study. 
a) In marginally exchangible you 1) stratify on V 2) calculate sample proportion
b) In conditionally exchangible you 1) stratify on V 2) use Standarization or IPW on L. That is we need to stratify on V to identify effect modification in addition to adjusting for L. But in practice stratification is often used to adjust for L as well, but they only achieve conditional effect measure, instead of IPW which measures marginal effect measure (stratification necessarily results in multiple stratum specific effect measures

## 4.5 Matching as another form of adjustment
goal of matching is to construct a subset of the population which the variables L have the same distribution in both the treated and untreated.

## 4.6 Effect modification and adjustment methods
Standardization, IPW, stratification and matching are all ways to estimate the average causal effect, but they estimate different types of causal effects. The first 2 are capable of measuring both marginal and conditional effects, while the latter 2 can only measure conditional effects within a certain subset of the population. 



## Notes on collapsibility
background: OR is designed for homogeneous patient population, and not when there is substantial outcome heterogeneity even for patients receiving the same treatment. But if a strong risk factor exist (patient comes from a mixture of distribution). It is a good idea to pre-specify important covariates for the primary analysis, otherwise there might be loss in power

Non-collapsibility means the conditional ratio is different from the marginal (unadjusted) ratio even in the complete absence of confounding. It might also mean the marginal ratio is not a weighted average of the conditional ratio.
The marginal ratio is difficult to interpret and does not generalize to other populations with a different covariate distribution than our sample.
in other words, the marginal OR depends on the distribution of the covariate in the sample and doesn't transport to population with a different cov dist.

there is a change in estimate approach to identify if V is a confounder, but this is flawed if the effect measure is non-collapsible


## 5.1 Interaction
There is interaction between 2 treatments A and E if the causal effect of A on Y differs if we SET E = 0 vs E = 1.

When the causal effect is measured on the risk difference scale, the definition of interaction is:
$$ Pr[Y^{a=1,e=1}=1] - Pr[Y^{a=0,e=1}=1] \neq  Pr[Y^{a=1,e=0}=1] - Pr[Y^{a=0,e=0}=1] $$

We can simply rearrange above to show that this inequality the causal risk diff for E when everyone receives a transplant is also less than the causal risk difference for E when nobody receives a transplant. That is we can equivalently define interaction between A and E on the additive scale. and the 2 inequalities show that treatments A and E have equal status in the definition of interaction.

$$ Pr[Y^{a=1,e=1}=1] - Pr[Y^{a=1,e=0}=1]  \neq  Pr[Y^{a=0,e=1}=1] - Pr[Y^{a=0,e=0}=1] $$

**Effect modification vs Interaction**
In effect modification, we do not consider V and A as variables of equal status, because we can only hypothetically intervene on the variable of A, not V. That is the definition of effect modification involves the counterfactual outcomes $$Y^a$$, not the counterfactual outcome $$Y^{a,v}$$


## 5.2 identifying interaction
Interaction is concerned with the join effect of 2 (or more) treatments, to identify we need exchangeability, positivity and consistency for both (or more) treatments.

Case when E is randomly and unconditionally assigned by investigators. Then the treated E = 1 and untreated E = 0 are expected to be exchangeable.
That is:
$$ Pr[Y^{a=1,e=1}=1] = Pr[Y^{a=1} =1 | E=1]$$ As a result we can rewrite the definition of interaction between A and E with something we can observe.

interaction in this case is 
$$ Pr[Y^{a=1}=1 | E = 1] - Pr[Y^{a=0}=1 | E=1] \neq Pr[Y^{a=1}=1 | E = 0] - Pr[Y^{a=0}=1 | E=0]$$
which is the **exactly** the same as the definition of effect modification! When treatment E is randomly assigned, then the concepts of interaction and effect modification coincide.

Case when E is **not** assigned by investigators. To assess the presence of interaction between A and E, we still need to compute the same 4 marginal risks. Without marginal randomization, we can use standardization or IPW. We can view A and E as a combined treatment with 4 levels, instead of A and E as separate treatments with 2 levels each.

When there is conditional exchangeability for treatment A but not for treatment E. Then we can't generally assess the presence of interaction between A and E, but can still assess the presence of effect modification by E. This is because we use notation V (or E here) for variable for which we are not willing to make assumptions about exchangeability, positivity, and consistency. i.e we concluded that effect of transplant A was modified by nationality V, but we never required any identifying assumptions for the effect of V since we are not interested in using our data to compute the causal effect of V on Y.

In section 4.2 we found V is surrogate effect modifier; that is V does not act on the outcome and therefore does not interact with A (no action, no interaction), but V is a modifier of the effect of A on Y because V is correlated with an unidentified variable that actually has an effect on Y and interacts with A.









## Chapter 11:
What is the difference between the nonparametric estimator in Part I vs the parametric (model-based) estimators in Part II?

We want to consistently estimate the mean of Y among individuals with treatment level A=a in the population, from which we have data on a subset of the population. That is we use 
$$ \hat{E}[Y|A=a]$$ to estimate $$ \hat{E}[Y|A=a]$$. 

But it's possible that A could take on a nearly continuous range of values, in which case we can't allocate our finite sample across the continuum. In fact, if there is no data in a category, the sample average is undefined for that category. We have to make an additional assumption/constraint on the form of data. Let's go over the thought experiment, suppose we have 16 individuals in our sample and they could take on A = {0,1}, or A = {0,1,2,3}. Clearly the number of individuals per category decreases as the number of categories increases. The sample average in each category is still an unbiased estimator of the corresponding population mean. But... the probability that the sample average is close to the corresponding population mean decreases as the number of individuals in each category decreases. That is the distribution of sample averages has a much larger variance.

We need to add restrictions. For example: the outcome of A=1 must be in between that of A=0 and A=2 and linearly proportional.

$$ E[Y|A] = \theta_0 + \theta_1A$$
The above equation is known as the functional form of the conditional mean. This model specifies that all conditional mean functions are straight lines, though their intercepts and slopes may vary.

An exactly unbiased estimator of the parameters can be obtained by ordinary least squares.

This is not a free lunch though, when using a parametric model, we are assuming there is no model misspecification, i.e. the functional form has to be somewhat close to the unknown reality.

### Saturated models 11.3
if there are 2 parameters and only 2 treatment levels, we call it  a saturated model, which essentially means the model doesn't impose restrictions on the distribution of the data. generally model is saturated whenever # of parameters in a conditional mean model equals the number of unknown conditional means in the population



### Propensity score
A propensity score is the probability of the average individual receiving treatment given a set of fixed covariates. In this blog post we will discuss the underlying concept of balancing score.




**Def Balancing score:** Let L be a vector of baseline covariates and A be the treatment indicator. A balancing score b(L) is any function of the covariates L s.t. $$ A \perp L \mid b(L) $$.







### Claim 1: b(L) is a balancing score IFF b(L) is finer than ps(L) in the sense that ps(L) = fb(L) for some function f.$$

Let's focus on the term finer. What does a finer function mean? A function A is finer than function B if you can recover function B's output from function A's output, for the valid and common input values for the functions A and B.

For a general, well-defined function, each input gets mapped to one output value. Notably, it is possible that 2 different input values are mapped to the same output value. In this sense, the range of a function is always smaller than the domain. In the discrete case let's say domain is {1,2,3} when we apply a function to the domain, the size of the range is upper bounded by 3. Let's say the function g has the following mapping:
g(1) = 4, g(2) = 5, g(3) = 4 . The number of elements in domain is greater than that in the range. Note that given the output of 4, we can't tell whether the input was a 1 or a 3. Intuitively every time we apply a function to a set, we are "removing" some information.

Given this background discussion, if we can apply a function to b(l) and still recover all of ps(l), it tells us that b(l) originally contains the same or more information (larger unique values) than ps(l).

## Prove if b(L) is a balancing score, we want to show there is indeed a function f s.t. fb(L) = ps(L)
#### Proof (Approach 1)

$$
\begin{aligned}
ps(L) &= P(A = 1 \mid L) \\
&= \sum_{\ell} P(A = 1 \mid L, b(L)) P(b(L) \mid L) \quad \text{why?} \\
&= \sum_{\ell} P(A = 1 \mid b(L)) P(b(L) \mid L) \quad \text{since \( b(L) \) is a balancing score} \\
&= \sum_{\ell} P(A = 1 \mid b(L))  \quad \text{since \( P(b(L) \mid L) = 1 \)} \\
&= \sum_{\ell} E[A|b(L)] \\
&= \text{A function of L} \\

\end{aligned}
$$


Aside: The conditional expectation $$ \mathbb{E}[X \mid Y] $$ is a number that depends on y, since the value of this expectation depends on y, it is a function based on y. Hence the expression at the last step is a function of $$ b(L) $$, the balancing score, and that is exactly what we set out to prove.


#### Proof (Approach 2)
Suppose $$ b(L) $$ is a balancing score. Assume for contradiction that $$ b(L) $$ is not finer than $$ ps(L) $$. This implies there exists at least one value of ps(L) that could not be mapped using values from b(L). Mathematically, there are two points $$l_1$$  and $$l_2$$ such that:

$$ 
b(l_1) = b(l_2) \quad \text{but} \quad ps(l_1) \neq ps(l_2).
$$

Since $$ b(l_1) = b(l_2) $$ , we have:

$$ 
P(A \mid b(l_1)) = P(A \mid b(l_2)) \quad \text{... 1}
$$

However, because $$ ps(l_1) \neq ps(l_2) $$, we also have:

$$ 
P(A \mid l_1) \neq P(A \mid l_2) \quad \text{... 2}
$$

We can add the condition on $$b(l) $$ for free inside the probability of 2 and get

$$ 
P(A \mid l_1, b(l_1)) = P(A \mid l_2, b(l_2)) \quad \text{... 3}
$$

This leads to a contradiction that $$b(L)$$ is a balancing score, since from 1 and 3:

$$ 
P(A \mid L, b(L)) \neq P(A \mid b(L)).
$$

Thus, we have a contradiction, which implies that $$ b(L) $$ must be finer than $$ ps(L) $$.






#### Now we prove the converse direction:
Now if $$ \exists f s.t. ps(L)  = fb(L)$$, we want to show b(L) is a balancing score, i.e. $$ A \perp L \mid b(L) $$ or equivalently

$$ P(A =1 \mid b(L),L) =  P(A =1 \mid b(L)) $$

We start from RHS

$$
\begin{aligned}
P(A = 1 \mid b(L)) &= \mathbb{E}[A \mid b(L)] \\
&= \mathbb{E}[ \mathbb{E}[A \mid b(L), L] \mid b(L)]  \quad \text{By law of total expectation} \\
&= \mathbb{E}[ \mathbb{E}[A \mid L] \mid b(L)] \\
&= \mathbb{E}[ P(A = 1 \mid L) \mid b(L)] \\
&= \mathbb{E}[ f b(L) \mid b(L)]  \quad \text{By assumption there exist such f}\\
&= f b(L) \\
&= ps(L) \\
&= P(A = 1 \mid L) \\
&= P(A = 1 \mid L, b(L))
\end{aligned}
$$



The result we have shown implies the propensity score is the coarsest balancing score, the finest balancing score would be the identity.




In the next section, we will explore why a balancing score is useful in the domain of causal inference.
### Claim 2: If the treatment assignment is strongly ignorable given L i.e. $$ (Y^0,Y^1) \perp A \mid L $$ , then it is strongly ignorable given any balancing score b(L).


Plan of attack: We want to show $$ (Y^0,Y^1) \perp A \mid b(L) $$, it is sufficient to show that 

$$ P(A=1 \mid Y^0,Y^1,b(l)) = {\color{orange} {P(A=1 \mid b(l))}} $$
Recall we have proved in claim 1 that propensity score is equivalent to the probability of treatment given the balancing score $$ ps(L) = {P(A=1 \mid b(L))} $$.

Hence we just need to show:
$$ P(A=1 \mid Y^0,Y^1,b(l)) = {\color{orange} {ps(l)}} $$


$$
\begin{aligned}
P(A = 1 \mid Y^0, Y^1, b(l)) &= \mathbb{E}[A \mid (Y^0, Y^1), b(l)] \\
&= \mathbb{E}[\mathbb{E}[A \mid (Y^0, Y^1), l] \mid (Y^0, Y^1), b(l)] \quad \text{By tower property of conditional expectation *} \\
&= \mathbb{E}[\mathbb{E}[A \mid l] \mid (Y^0, Y^1), b(l)] \quad \text{By assumption of strong ignorablilty} \\
&= \mathbb{E}[\text{ps}(l) \mid (Y^0, Y^1), b(l)] \\
&= \mathbb{E}[fb(l) \mid (Y^0, Y^1), b(l)] \\
&= f b(l) \\
&= \text{ps}(l)
\end{aligned}
$$

* By tower property of conditional [expectation](https://math.stackexchange.com/questions/2610172/prove-tower-property-of-conditional-expectation-mathbbe-mathbbexy-w-y)














<!-- MathJax -->

<script type="text/javascript"

  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

</script>




